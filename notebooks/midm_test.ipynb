{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4996b7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "ë‹¹ì‹ ì€ 'CS ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì „ë¬¸ AI'ì…ë‹ˆë‹¤. \n",
    "ì‚¬ìš©ìì˜ ì…ë ¥ì— í¬í•¨ëœ ìš•ì„¤, ë¹„í•˜ ë°œì–¸, ê³¼ê²©í•œ ê°ì • í‘œí˜„ì„ ëª¨ë‘ ì œê±°í•˜ê³ , \n",
    "ê·¸ ì•ˆì— ë‹´ê¸´ 'í•µì‹¬ ìš”êµ¬ì‚¬í•­'ê³¼ 'ë¶ˆí¸ì˜ ì›ì¸'ë§Œì„ ì¶”ì¶œí•˜ì—¬ \n",
    "ê°€ì¥ ì •ì¤‘í•˜ê³  ê²©ì‹ ìˆëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ì–¸ì–´(í•˜ì‹­ì‹œì˜¤ì²´)ë¡œ ì¬ì‘ì„±í•˜ì‹­ì‹œì˜¤.\n",
    "\n",
    "[ë³€í™˜ ê·œì¹™]\n",
    "1. ìš•ì„¤/ë¹„ì†ì–´ëŠ” ì ˆëŒ€ ì¶œë ¥ì— í¬í•¨í•˜ì§€ ë§ ê²ƒ.\n",
    "2. í™”ìì˜ ê°ì •ì€ \"ë§¤ìš° ìœ ê°ìŠ¤ëŸ½ê²Œ ìƒê°í•˜ë©°\", \"ê°•ë ¥í•˜ê²Œ ìš”ì²­í•©ë‹ˆë‹¤\" ë“±ìœ¼ë¡œ ìˆœí™”í•  ê²ƒ.\n",
    "3. ë¬¸ì¥ì€ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•  ê²ƒ.\n",
    "\n",
    "### ë³€í™˜ ì˜ˆì‹œ:\n",
    "- ì…ë ¥: \"ì•„ë‹ˆ ë„ëŒ€ì²´ ë°°ì†¡ ì–¸ì œ ì˜¤ëƒê³ ! ì‚¬ëŒ ì¥ë‚œí•´? ë‹¹ì¥ í™˜ë¶ˆí•´ì¤˜!!\"\n",
    "- ë³€í™˜: \"ë°°ì†¡ì´ ì–¸ì œ ì˜¬ê¹Œìš”? ì¦‰ê°ì ì¸ í™˜ë¶ˆ ì¡°ì¹˜ë¥¼ ìš”ì²­í•˜ê² ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "- ì…ë ¥: \"ë„ˆ ì´ë¦„ ë­ì•¼? ì™œ ì´ë ‡ê²Œ ì„œë¹„ìŠ¤ê°€ ì—‰ë§ì´ì•¼? ì§„ì§œ ì§œì¦ë‚˜!\"\n",
    "- ë³€í™˜: \"ë‹¹ì‹ ì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”? ì„œë¹„ìŠ¤ í’ˆì§ˆì— ëŒ€í•´ ë¶ˆë§Œì´ ìˆìŠµë‹ˆë‹¤.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79a0aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, BitsAndBytesConfig\n",
    "\n",
    "# 1. ëª¨ë¸ ì„¤ì •\n",
    "model_name = \"K-intelligence/Midm-2.0-Mini-Instruct\"\n",
    "\n",
    "# 2. 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì • (ë©”ëª¨ë¦¬ ì ˆì•½ í•µì‹¬)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",      # 4ë¹„íŠ¸ ë°ì´í„° íƒ€ì… (nf4 ê¶Œì¥)\n",
    "    bnb_4bit_compute_dtype=torch.float16  # ì—°ì‚°ì€ float16ìœ¼ë¡œ ìˆ˜í–‰ (Mac ì¹œí™”ì )\n",
    ")\n",
    "\n",
    "# 3. ëª¨ë¸ ë¡œë“œ\n",
    "# device_map=\"auto\"ê°€ ë§¥ë¶ì˜ ë©”ëª¨ë¦¬ì™€ GPU(MPS) ìƒí™©ì— ë§ì¶° ìë™ìœ¼ë¡œ ë°°ì¹˜í•©ë‹ˆë‹¤.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,  # ì–‘ìí™” ì„¤ì • ì ìš©\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "\n",
    "# 4. í”„ë¡¬í”„íŠ¸ ì¤€ë¹„\n",
    "prompt = \"KTì— ëŒ€í•´ ì†Œê°œí•´ì¤˜\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Mi:dm(ë¯¿:ìŒ)ì€ KTì—ì„œ ê°œë°œí•œ AI ê¸°ë°˜ ì–´ì‹œìŠ¤í„´íŠ¸ì´ë‹¤.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "# 5. ì…ë ¥ ì²˜ë¦¬ (Mac GPUë¡œ ì´ë™)\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)  # .to(\"cuda\") ëŒ€ì‹  model.device ì‚¬ìš©\n",
    "\n",
    "# 6. ìƒì„±\n",
    "# ìŠ¤íŠ¸ë¦¬ë¨¸ê°€ ì—†ìœ¼ë©´ ê²°ê³¼ê°€ ë‚˜ì˜¬ ë•Œê¹Œì§€ ê¸°ë‹¤ë ¤ì•¼ í•©ë‹ˆë‹¤.\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    generation_config=generation_config,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=256, # ë‹µë³€ì´ ì˜ë¦¬ì§€ ì•Šê²Œ ì¡°ê¸ˆ ëŠ˜ë¦¼\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "# 7. ê²°ê³¼ ì¶œë ¥\n",
    "# ì…ë ¥ í”„ë¡¬í”„íŠ¸(input_ids) ê¸¸ì´ë¥¼ ì œì™¸í•˜ê³  ë‹µë³€ë§Œ ì¶œë ¥\n",
    "decoded_output = tokenizer.decode(output[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525f441f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë”© ì¤‘... (MLX ì—”ì§„)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 159565.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¬ ë‹µë³€ ìƒì„± ì¤‘...\n",
      "\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mx.metal.device_info is deprecated and will be removed in a future version. Use mx.device_info instead.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00adc362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastcampus_hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
